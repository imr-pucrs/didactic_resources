{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "In this part of the exercise, you will implement a neural network to recognize handwritten digits using the MNIST dataset. The neural network will be able to represent complex models that form non-linear hypotheses.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "You are given a data set in [`data_nn.npy`](https://goo.gl/GhGLAQ) that contains 5000 training examples of handwritten digits (This is a subset of the [MNIST](http://yann.lecun.com/exdb/mnist/) handwritten digit dataset. The `.npy` format means that that the data has been saved in a native Numpy matrix format, instead of a text (ASCII) format like a csv-file. These matrices can be read directly into your program by using the load command. After loading, matrices of the correct dimensions and values will appear in your program's memory.\n",
    "\n",
    "There are 5000 training examples in `data_nn.npy`, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is \"unrolled\" into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix `X`. This gives us a 5000 by 400 matrix `X` where every row is a training example for a handwritten digit image.\n",
    "\n",
    "The second part of the training set is a 5000-dimensional vector `y` that contains labels for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset into matrices X and y\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(filedata):\n",
    "    \"\"\" Load the content from data_nn.npy \"\"\"\n",
    "    data = np.load(filedata).item()\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    return X, y\n",
    "\n",
    "X, y = load_dataset('data/data_nn.npy')\n",
    "X = np.matrix(X)\n",
    "y = np.matrix(y)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing labels\n",
    "\n",
    "As classification predicts scores for each class, first we have to convert the value of each class into a one-hot-encoding vector containing the value of the class. Thus, for a class `0` in our dataset, we represent it as one hot vector:\n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "Class `1` in our dataset is represented as:\n",
    "\n",
    "\n",
    "```\n",
    "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert labels into one hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False) # sparse=False return an array\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "y_onehot = np.matrix(y_onehot)\n",
    "y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data\n",
    "\n",
    "First thing you should do, is visualize the data loaded into `X`. To do so, select randomly 100 rows from `X` and map each row to a 20 pixel by 20 pixel grayscale image and display the images together. The resulting image should be like:\n",
    "\n",
    "<img src='images/mnist.png' width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def display_image(X):\n",
    "    \"\"\" From matrix X, display 100 random images into a single figure\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "display_image(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model representation\n",
    "\n",
    "Our neural network is shown in the image below. It has 3 layers - an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values of digit images. Since the images are of size 20Ã—20, this gives us 400 input layer units (excluding the extra bias unit which always outputs +1). \n",
    "\n",
    "You have been provided with a set of network parameters ($\\theta^{(1)}$ , $\\theta^{(2)}$) already trained. These are stored in [`weights_nn.npy`](https://goo.gl/grDJAS) and will be loaded into `theta1` and `theta2`. The parameters have dimensions that are sized for a neural network with 25 units in the second layer and 10 output units (corresponding to the 10 digit classes).\n",
    "\n",
    "<img src=\"images/neuralnet.png\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weights(filedata):\n",
    "    \"\"\" Load the content from data_nn.npy \"\"\"\n",
    "    data = np.load(filedata).item()\n",
    "    theta1 = data['theta1']\n",
    "    theta2 = data['theta2']\n",
    "    return theta1, theta2\n",
    "\n",
    "theta1, theta2 = load_weights('data/weights_nn.npy')\n",
    "theta1 = np.matrix(theta1)\n",
    "theta2 = np.matrix(theta2)\n",
    "print(theta1.shape)\n",
    "print(theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Propagation and Prediction\n",
    "\n",
    "Now you will implement feedforward propagation for the neural network. You should implement the feedforward computation that computes $h_\\theta(x^{(i)})$ for every example `i` and returns the associated predictions, where the prediction from the neural network will be the label that has the largest output $(h_\\theta(x))_k$.\n",
    "\n",
    "**Implementation Note**: The matrix `X` contains the examples in rows, then you will need to add the column of 1's to the matrix. The matrices `Theta1` and `Theta2` contain the parameters for each unit in rows. Specifically, the first row of `Theta1` corresponds to the first hidden unit in the second layer. When you compute $z^{(2)} = \\theta^{(1)}a^{(1)}$, be sure that you index (and if necessary, transpose) `X` correctly so that you get a (`l`) as a column vector.\n",
    "\n",
    "Remember that the sigmoid function is defined as:\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "If your predict function using the loaded set of parameters for `Theta1` and `Theta2` is correct, you should see that the accuracy is about 97.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\" Activation function \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    # return g\n",
    "\n",
    "def forward(X, theta1, theta2):\n",
    "    \"\"\" \n",
    "    Apply the forward propagation \n",
    "    To apply backpropagation, the function should return\n",
    "    the values of a1, z2, a2, z3 and h\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    #return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the forward function\n",
    "a1, z2, a2, z3, h = forward(X, theta1, theta2)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.array(np.argmax(h, axis=1)+1)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "\n",
    "print('Accuracy with loaded weights must be 97.52%')\n",
    "print('Predicted accuracy = %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Now you will implement the cost function and gradient for the neural network. Recall that the cost function for the neural network is\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} [-y_k^{(i)}log((h_\\Theta(x^{(i)}))_k-(1 - y_k^{(i)})log(1 - h_\\Theta(x^{(i)})_k)]$$\n",
    "\n",
    "where K = 10 is the total number of possible labels. Note that $h_\\theta(x^{(i)})_k = a_k^{(3)}$ is the activation (output value) of the k-th output unit. Also, recall that whereas the original labels (in the variable `y`) were 1, 2, ..., 10, for the purpose of training a neural network, we need to recode the labels as vectors containing only values 0 or 1, so that\n",
    "\n",
    "$$y = \\begin{bmatrix}\n",
    "1 \\\\ \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "\\vdots \\\\ \n",
    "0 \\\\\n",
    "\\end{bmatrix},  \\ \\ \\ \\\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ \n",
    "1 \\\\ \n",
    "0 \\\\ \n",
    "\\vdots \\\\ \n",
    "0 \\\\\n",
    "\\end{bmatrix},  \\ \\ \\ \\\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "1 \\\\ \n",
    "\\vdots \\\\ \n",
    "0 \\\\\n",
    "\\end{bmatrix},  \\ \\ \\ \\ \\dots \\text{or} \\ \\ \\ \\\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "\\vdots \\\\ \n",
    "1 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "For example, if $x^{(i)}$ is an image of the digit 5, then the corresponding $y^{(i)}$ (that you should use with the cost function) should be a 10-dimensional vector with $y_5 = 1$, and the other elements equal to 0. You should implement the feedforward computation that computes $h_\\theta(x^{(i)})$ for every example `i` and sum the cost over all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(X, y, theta1, theta2):\n",
    "    \"\"\" Verify the cost using theta1 and theta2 \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    # return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test cost function\n",
    "J = cost_function(X, y_onehot, theta1, theta2)\n",
    "print('Your current cost should be: 0.287629')\n",
    "print('Current cost : %f' % J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "Now, you will implement the backpropagation algorithm to compute the gradient for the neural network cost function. Once you have computed the gradient, you will be able to train the neural network by minimizing the cost function $J(\\theta)$ using an optimizer such as the `fmin_tnc` from Scipy.\n",
    "\n",
    "You will first implement the backpropagation algorithm to compute the gradients for the parameters for the neural network.\n",
    "\n",
    "## Sigmoid gradient\n",
    "\n",
    "First you have to implement the sigmoid gradient function. The gradient for the sigmoid function can be computed as\n",
    "\n",
    "$$g'(z) = \\frac{d}{dz}g(z) = g(z)(1 - g(z))$$\n",
    "\n",
    "where\n",
    "\n",
    "$$sigmoid(z) = g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "When you are done, try testing a few values by calling `sigmoid_gradient(z)`. For large values (both positive and negative) of `z`, the gradient should be close to 0. When z = 0, the gradient should be exactly 0.25. Your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid gradient function on every element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    \"\"\" Computes the gradient of the sigmoid function \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    # return dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the gradient of sigmoid\n",
    "print('Computing gradient for [1, -0.5, 0, 0.5, 1]')\n",
    "print('Correct gradients:    [ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]')\n",
    "\n",
    "values = np.array([1, -0.5, 0, 0.5, 1])\n",
    "sgrad = sigmoid_gradient(values)\n",
    "\n",
    "print('Calculated gradients: %s' % str(sgrad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random initialization\n",
    "\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{(l)}$ uniformly in the range [$-\\epsilon_{init}$ , $\\epsilon_{init}$]. You should use init = 0.12 (One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is $\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in}+L_{out}}}$, where $L_{in} = s_l$ and $L_{out} = s_{l+1}$ are the number of units in the layers adjacent to $\\Theta^{(l)}$.) This range of values ensures that the parameters are kept small and makes the learning more efficient. \n",
    "\n",
    "Thus, consider a matrix of weights of 10x11, a random initialization should be performed as\n",
    "\n",
    "$$Theta = rand(10, 11) \\times (2\\epsilon) - \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(L_in, L_out):\n",
    "    \"\"\" \n",
    "    Randomly initialize the weights of a layer with \n",
    "    L_in income connections and L_out outgoing connections\n",
    "    \n",
    "    Note: W should be set to a matrix of size(L_out, 1+L_in) as\n",
    "          the column row of W handles the 'bias' terms.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    #return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Initializing Neural Network Parameters...')\n",
    "# initial setup :: set constants\n",
    "NB_HIDDEN = 25\n",
    "LEARNING_RATE = 1.0\n",
    "input_layer_size = X.shape[1]\n",
    "num_labels = y_onehot.shape[1]\n",
    "\n",
    "initial_Theta1 = initialize_weights(input_layer_size, NB_HIDDEN)\n",
    "initial_Theta2 = initialize_weights(NB_HIDDEN, num_labels)\n",
    "\n",
    "thetas = np.concatenate((np.ravel(initial_Theta1), np.ravel(initial_Theta2)))\n",
    "\n",
    "print('Input layer size: %d' % input_layer_size)\n",
    "print('Number of hidden units: %d' % NB_HIDDEN)\n",
    "print('Number of labels: %d' % num_labels)\n",
    "print('theta1 shape: (%d, %d)' % initial_Theta1.shape)\n",
    "print('theta2 shape: (%d, %d)' % initial_Theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Recall that the intuition behind the backpropagation algorithm is as follows. Given a training example ($x^{(t)}, y^{(t)}$), we will first run a \"forward pass\" to compute all the activations throughout the network, including the output value of the hypothesis $h_\\theta(x)$. Then, for each node `j` in layer `l`, we would like to compute an \"error term\" $\\delta_j^{(l)}$ that measures how much that node was \"responsible\" for any errors in our output.\n",
    "\n",
    "For an output node, we can directly measure the difference between the network's activation and the true target value, and use that to define $\\delta_j^{(3)}$ (since layer 3 is the output layer). For the hidden units, you will compute $\\delta_j^{(l)}$ based on a weighted average of the error terms of the nodes in layer $(l+1)$.\n",
    "\n",
    "In detail, here is the backpropagation algorithm (also depicted in the image below). You should implement steps 1 to 4 in a loop that processes one example at a time. Concretely, you should implement a for-loop `for t in range(1:m)` and place steps 1-4 below inside the for-loop, with the t-th iteration performing the calculation on the t-th training example ($x^{(t)}, y^{(t)}$). Step 5 will divide the accumulated gradients by $m$ to obtain the gradients for the neural network cost function.\n",
    "\n",
    "<img src=\"images/backprop.png\", width=\"40%\"/>\n",
    "\n",
    "1) Set the input layerâ€™s values ($a^{(1)}$) to the t-th training example $x^{(t)}$. Perform a feedforward pass (Figure above), computing the activations ($z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)}$) for layers 2 and 3. Note that you need to add a $+1$ term to ensure that the vectors of activations for layers $a^{(1)}$ and $a^{(2)}$ also include the bias unit.\n",
    "\n",
    "2) For each output unit $k$ in layer 3 (the output layer), set\n",
    "\n",
    "$$\\delta_k^{(3)} = (a_k^{(3)} - y_k)$$\n",
    "\n",
    "where $y_k \\in \\{0, 1\\}$ indicates whether the current training example belongs to class $k (y_k = 1)$, or if it belongs to a different class $(y_k = 0)$. You may find logical arrays helpful for this task (explained in the previous programming exercise).\n",
    "\n",
    "3) For the hidden layer $l=2$, set\n",
    "\n",
    "$$\\delta^{(2)} = (\\Theta^{(2)})^T\\delta^{(3)} .* g'(z^{(2)})$$\n",
    "\n",
    "4) Accumulate the gradient from this example using the following formula. Note that you should skip or remove $\\delta_0^{(2)}$ .\n",
    "\n",
    "$$\\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)}(a^{(1)})^T$$\n",
    "\n",
    "5) Obtain the gradient for the neural network cost function by dividing the accumulated gradients by $\\frac{1}{m}$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)}$$\n",
    "\n",
    "**Tip**: You should implement the backpropagation algorithm only after you have successfully completed the feedforward and cost functions. While implementing the backpropagation algorithm, it is often useful to use the size function to print out the sizes of the variables you are working with if you run into dimension mismatch errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backpropagation function\n",
    "def backpropagation(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
    "    \"\"\" \n",
    "    Apply backpropagation algorithm on the input. Backpropagation has this signature\n",
    "    due to `scipy.optimize.minimize` function that will perform it automatically.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "    #grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "    #return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform forward-backpropagation, updating thetas to minimize the cost function\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Minimize a scalar function of one or more variables using a truncated Newton (TNC) algorithm\n",
    "# If jac is a Boolean and is True, fun is assumed to return the gradient along with the objective function\n",
    "fmin = minimize(fun=backpropagation, x0=thetas, \n",
    "                args=(input_layer_size, NB_HIDDEN, num_labels, X, y_onehot, LEARNING_RATE), \n",
    "                method='TNC', jac=True, options={'maxiter': 250})\n",
    "fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the new accuracy for the trained model\n",
    "y_pred = np.array(np.argmax(h, axis=1)+1)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "\n",
    "print('Accuracy with loaded weights must be 97.52%')\n",
    "print('Predicted accuracy = %.2f' % (accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
